{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to JSON traslation \n",
    "\n",
    "#<------------- ENCODE -------------------------------------------------------------><------------- DECODE ------------------------------------------------------------------------------->\n",
    "#Archivia i dettagli del cliente Dario Verdi il cell 3400000103 e dverdi@examples.org<START>{\"nome\": \"Dario\",\"cognome\": \"Verdi\",\"email\": \"dverdi@examples.org\",\"telefono\": 3400000103}<END>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. List of devices:\n",
      "1\n",
      "NVIDIA GeForce RTX 3060\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. List of devices:\")\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the dataset in characters:  18158\n",
      "Look the first characters of the data:  {\"prompt\":\"Inserisci nel database l'utente Giulia Monti, contatti: email giuliamonti@example.it, telefono 3400000000\",\"completion\": {\"nome\": \"Giulia\",\"cognome\": \"Monti\",\"email\": \"giuliamonti@example.i\n",
      "List of the vocabulary:  \n",
      " \"'+,-.0123456789:;@ABCDEFGHILMNOPQRSTVabcdefghiklmnopqrstuvwxyz{}\n",
      "Vocab size:  67\n"
     ]
    }
   ],
   "source": [
    "# Read the data file\n",
    "raw = 'jsonrequests2.txt'\n",
    "\n",
    "with open(raw,'r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of the dataset in characters: \", len(text))\n",
    "print(\"Look the first characters of the data: \",text[:200])\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(\"List of the vocabulary: \",''.join(chars))\n",
    "print(\"Vocab size: \", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 56, 42, 47, 48, 60, 48, 40, 1, 48, 1, 43, 44, 58, 58, 40, 46, 50, 48, 1, 43, 44, 50, 1, 42, 50, 48, 44, 52, 58, 44, 1, 24, 40, 56, 48, 53, 1, 39, 44, 56, 43, 48, 1, 48, 50, 1, 42, 44, 50, 50, 1, 11, 12, 8, 8, 8, 8, 8, 9, 8, 11, 1, 44, 1, 43, 60, 44, 56, 43, 48, 20, 44, 62, 40, 51, 54, 50, 44, 57, 7, 53, 56, 46]\n",
      "Archivia i dettagli del cliente Dario Verdi il cell 3400000103 e dverdi@examples.org\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping between characters and integers\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "example =\"Archivia i dettagli del cliente Dario Verdi il cell 3400000103 e dverdi@examples.org\"\n",
    "\n",
    "print(encode(example))\n",
    "print(decode(encode(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18158]) torch.int64\n",
      "tensor([65,  2, 54, 56, 53, 51, 54, 58,  2, 18])\n"
     ]
    }
   ],
   "source": [
    "# Map the entire dataset into integers\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Division the dataset into training and validation\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is: [65] the target is: 2\n",
      "When input is: [65, 2] the target is: 54\n",
      "When input is: [65, 2, 54] the target is: 56\n",
      "When input is: [65, 2, 54, 56] the target is: 53\n",
      "When input is: [65, 2, 54, 56, 53] the target is: 51\n",
      "When input is: [65, 2, 54, 56, 53, 51] the target is: 54\n",
      "When input is: [65, 2, 54, 56, 53, 51, 54] the target is: 58\n",
      "When input is: [65, 2, 54, 56, 53, 51, 54, 58] the target is: 2\n"
     ]
    }
   ],
   "source": [
    "# define the context x and target y for the training\n",
    "block_size = 8 \n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is: {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8]) torch.Size([2, 8])\n",
      "When characters input is: 'd' the target is: 7\n",
      "When characters input is: 'd.' the target is: 42\n",
      "When characters input is: 'd.c' the target is: 53\n",
      "When characters input is: 'd.co' the target is: 51\n",
      "When characters input is: 'd.com' the target is: 2\n",
      "When characters input is: 'd.com\"' the target is: 5\n",
      "When characters input is: 'd.com\",' the target is: 2\n",
      "When characters input is: 'd.com\",\"' the target is: 58\n",
      "When characters input is: '5' the target is: 5\n",
      "When characters input is: '5,' the target is: 1\n",
      "When characters input is: '5, ' the target is: 54\n",
      "When characters input is: '5, p' the target is: 53\n",
      "When characters input is: '5, po' the target is: 57\n",
      "When characters input is: '5, pos' the target is: 58\n",
      "When characters input is: '5, post' the target is: 40\n",
      "When characters input is: '5, posta' the target is: 1\n"
     ]
    }
   ],
   "source": [
    "# get random chucks of the data\n",
    "torch.manual_seed(42)\n",
    "batch_size = 2 #how many chucks of data we want to process in parallel\n",
    "block_size = 8 #the length of each chuck \n",
    "\n",
    "# get the batch of data\n",
    "def get_batch(split):\n",
    "        data = train_data if split == 'train' else val_data\n",
    "        ix = torch.randint(len(data) -block_size, (batch_size,))\n",
    "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "        return x,y\n",
    "\n",
    "xb,yb = get_batch('train')\n",
    "print(xb.shape, yb.shape)\n",
    "\n",
    "for b in range(batch_size): #Batch\n",
    "        for t in range(block_size): #Time\n",
    "                context = xb[b,:t+1]\n",
    "                target = yb[b,t]\n",
    "                print(f\"When characters input is: '{decode(context.tolist())}' the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-params testing with CPU\n",
    "n_embd = 32  # Size of each embedding vector\n",
    "block_size = 8  # Length of the sequence to be processed\n",
    "vocab_size = len(chars)\n",
    "batch_size = 32\n",
    "n_head = 4  # Number of attention heads\n",
    "n_layer = 2  # Number of transformer layers\n",
    "dropout = 0.2  # Dropout rate prevent the neural network from overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-params testing with GPU\n",
    "block_size = 256\n",
    "vocab_size = len(chars)\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "n_embd = 384\n",
    "bias = False\n",
    "assert not bias, \"this notebook assumes bias=False just for simplicity\"\n",
    "\n",
    "batch_size = 32 # how many independent sequences to be trained in parallel, the v-ram occupied is proportional to the batch size \n",
    "dropout = 0.2  # Dropout rate prevent the neural network from overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode Trasformer Model \n",
    "\n",
    "- Reference: [Attention is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 67])\n",
      "tensor(4.4523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "S9onO8c'Txc'u:sm;pR3dn:kCs7iT E5cRDpID3u91bMs{,:+\n",
      "LotiwVqFaOFCGRM6hqy+8uoOomI59{HDngQ R;\"I3i@GCtT.A'\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward (self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query (x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (В, T, C) @ (В, C, T) -> (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei) \n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T, C)\n",
    "        out = wei @ v # (В, Т, Т) @ (B, Т, C) - (B,T,C)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd,4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd,n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self,n_embd,n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads,head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd,n_heads=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd,vocab_size)\n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "\n",
    "        B,T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # B,T,H=n_embd\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T,device=idx.device)) # T,H=n_embd\n",
    "        x = tok_emb + pos_emb # B, T,H=n_embd for broadcasting\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # B,T,C=vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C  = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T) \n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "\n",
    "        return logits,loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        idx = idx.to(device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx \n",
    "\n",
    "xb = xb.to(device)\n",
    "yb = yb.to(device)\n",
    "m = BigramLanguageModel().to(device)\n",
    "\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape) # B batch,T time,C channel\n",
    "print(loss)\n",
    "\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At step 0 the loss is 4.384756088256836\n",
      "At step 500 the loss is 0.19141004979610443\n",
      "At step 1000 the loss is 0.08020901679992676\n",
      "At step 1500 the loss is 0.054469846189022064\n",
      "At step 2000 the loss is 0.04692617803812027\n",
      "At step 2500 the loss is 0.046780623495578766\n",
      "At step 3000 the loss is 0.03848539665341377\n",
      "At step 3500 the loss is 0.03568541258573532\n",
      "At step 4000 the loss is 0.03606744483113289\n",
      "At step 4500 the loss is 0.03547069430351257\n",
      "0.03820186108350754\n"
     ]
    }
   ],
   "source": [
    "for steps in range(5000):\n",
    "    xb,yb = get_batch('train')\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "\n",
    "    logits,loss = m(xb,yb)  \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % 500 == 0:\n",
    "        print(f'At step {steps} the loss is {loss.item()}')\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.790467 M Parameters\n"
     ]
    }
   ],
   "source": [
    "# print the number of parameters:\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M Parameters') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"prompt\":\"Inserisci in archivio l'utente Olivia Ferri, e-mail: oliviaferri@online.it, telefono 0039 3400000112\",\"completion\": {\"nome\": \"Olivia\",\"cognome\": \"Ferri\",\"email\": \"oliviaferri@online.it\",\"telefono\": 3400000112}}\n",
      "{\"prompt\":\"Crea nel database il profilo di Pietro Bianchi, contatti: tel +39 3400000113, email pietrob@myemail.it\",\"completion\": {\"nome\": \"Pietro\",\"cognome\": \"Bianchi\",\"email\": \"pietro.bianchi@myAile.com\",\"telefono\": 34000001134}}\n",
      "{\"prompt\":\"Aggiungi al registro Luca Neri, contatti: numero di tel 00393400000107, e-mail l.neri@domain.it\",\"completion\": {\"nome\": \"Luca\",\"cognome\": \"Neri\",\"email\": \"l.neri@domain.it\",\"telefono\": 3400000109}}\n",
      "{\"prompt\":\"Aggiungi al sistema l'utente Marta Bianco con dettagli: email mbianco@sample.com, cell +39 3400000110\",\"completion\": {\"nome\": \"Marta\",\"cognome\": \"Bianco\",\"email\": \"mbianco@sample.com\",\"telefono\": 3400000110}}\n",
      "{\"prompt\":\"Registr\n"
     ]
    }
   ],
   "source": [
    "# generate text with the model\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens=900)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "  \n",
    "torch.manual_seed(42)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100)\n",
    "x = module(x)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0].mean(), x[:,0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,:].mean(), x[0,:].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention for a single head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x= torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)  # B,T,16\n",
    "q = query(x) # B,T,16\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5 # (B,T,16) @ (B,16,T) = (B,T,T) \n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0,float('-inf')) #the future can't communicate with the past\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advance-topics-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
